Index: src/staticnn/model/fixednn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\n\n# Tipi utili per chiarezza\nArray2D = np.ndarray\nArray1D = np.ndarray\n\n####        INIZIALIZZAZIONE RETE NEURALE STATICA VALORI HARDCODED FULL CONNECT         ####\n#\n#.  HIDDEN LAYER = 1 \n#.  UNITS = 28\n#   ---->  numero totale di pesi in una full connect è: \n#               numero unità input layer (12) * numero unità hidden layer (28 FIXED) = totale 336 unità\n#\n####    ####    ####    ####    ####    ####    ####    ####    ####    ####    ####    ####\n\n\ndef initialize_neuraln(x_i, d) -> tuple[Array1D, Array2D, Array2D, Array1D]:\n    \"\"\"\n    Costruisce una rete neurale fissata di un hidden layer\n\n    Args:\n        x_i: l'input layer\n        x_k: l'output layer\n        d: targets\n    \n    Ritorna:\n        x_i: l'input layer\n        w_ji: matrice dei pesi W verso l'hidden layer\n        w_kj:  \"\"   \"\"    \"\"   K verso l'output layer  \n        x_k: l'output layer\n    \"\"\"\n\n    # Numero features e dimensioni NN \n    n_inputs = x_i.shape[1] + 1  # 12 + bias\n    #n_inputs = x_i.shape[1]\n    n_hidden = 28                   # fissato\n    n_outputs = d.shape[1]          # 4\n\n    # Inizializzazione pesi\n    w_ji = np.random.uniform(low = -0.7, high = 0.7, size = (n_inputs, n_hidden)) # (12 × 28)\n    w_kj = np.random.uniform(low = -0.7, high = 0.7, size = (n_hidden, n_outputs)) # (28 × 4) \n\n    # BIAS\n    # Il bias deve essere aggiunto come un valore (= 1) in più sul vettore x -> x_0 (= 1) + x_1 + .... + x_n\n    # e come peso in più w_0. Questo funziona da treshold\n    rows = x_i.shape[0]\n    cols = x_i.shape[0]\n    bias = [[1] * 1 for _ in range(rows)]\n    x_ibiased = [[0] * cols for _ in range(rows)]\n\n    x_ibiased = np.hstack((x_i,bias))\n\n    #return x_i, w_ji, w_kj, d\n    return x_ibiased, w_ji, w_kj, d\n            \n\n\n\n\"\"\"\nrandom.uniform(low=0.0, high=1.0, size=None)\n\nDraw samples from a uniform distribution.\n\nSamples are uniformly distributed over the half-open interval [low, high) (includes low, but excludes high). In other words, any value within the given interval is equally likely to be drawn by uniform.\n\nNote\n\nNew code should use the uniform method of a Generator instance instead; please see the Quick start.\n\nParameters:\n\n    low\n    float or array_like of floats, optional\n\n        Lower boundary of the output interval. All values generated will be greater than or equal to low. The default value is 0.\n    high\n    float or array_like of floats\n\n        Upper boundary of the output interval. All values generated will be less than or equal to high. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). The default value is 1.0.\n    size\n    int or tuple of ints, optional\n\n        Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if low and high are both scalars. Otherwise, np.broadcast(low, high).size samples are drawn.\n\n\n\"\"\"
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/staticnn/model/fixednn.py b/src/staticnn/model/fixednn.py
--- a/src/staticnn/model/fixednn.py	(revision 2525b2dda23d528628ad70037169760047ca21b8)
+++ b/src/staticnn/model/fixednn.py	(date 1764251189053)
@@ -37,8 +37,8 @@
     n_outputs = d.shape[1]          # 4
 
     # Inizializzazione pesi
-    w_ji = np.random.uniform(low = -0.7, high = 0.7, size = (n_inputs, n_hidden)) # (12 × 28)
-    w_kj = np.random.uniform(low = -0.7, high = 0.7, size = (n_hidden, n_outputs)) # (28 × 4) 
+    w_ji = np.random.uniform(low = -0.07, high = 0.07, size = (n_inputs, n_hidden)) # (12+bias × 28)
+    w_kj = np.random.uniform(low = -0.07, high = 0.07, size = (n_hidden, n_outputs)) # (28 × 4)
 
     # BIAS
     # Il bias deve essere aggiunto come un valore (= 1) in più sul vettore x -> x_0 (= 1) + x_1 + .... + x_n
@@ -48,7 +48,7 @@
     bias = [[1] * 1 for _ in range(rows)]
     x_ibiased = [[0] * cols for _ in range(rows)]
 
-    x_ibiased = np.hstack((x_i,bias))
+    x_ibiased = np.hstack((bias,x_i))
 
     #return x_i, w_ji, w_kj, d
     return x_ibiased, w_ji, w_kj, d
Index: src/staticnn/training/backward/backward_pass.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nfrom src.staticnn.activationf.sigm import sigmaf\n#from src.staticnn.training.forward.forward_pass import forward_hidden, forward_output\ndef backprop_output(O_u, delta_t):\n    DelW=O_u * delta_t\n\n    return DelW
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/staticnn/training/backward/backward_pass.py b/src/staticnn/training/backward/backward_pass.py
--- a/src/staticnn/training/backward/backward_pass.py	(revision 2525b2dda23d528628ad70037169760047ca21b8)
+++ b/src/staticnn/training/backward/backward_pass.py	(date 1764251189053)
@@ -1,5 +1,6 @@
 import numpy as np
 from src.staticnn.activationf.sigm import sigmaf
+from src.staticnn.activationf.relu import *
 #from src.staticnn.training.forward.forward_pass import forward_hidden, forward_output
 def backprop_output(O_u, delta_t):
     DelW=O_u * delta_t
Index: src/staticnn/training/forward/forward_pass.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nfrom src.staticnn.activationf.sigm import sigmaf\n\n# Tipi utili per chiarezza\nArray2D = np.ndarray\nArray1D = np.ndarray\n\ndef forward_hidden(x_i: Array1D, w_ji: Array2D) -> Array1D:\n    \"\"\"\n    Calcola l'output del hidden layer con act. func. sigma\n    \n    Args: \n        X: vettore input (n_features)\n        W: matrice pesi (n_features, n_hidden)\n    \n    Ritorna: \n        vettore attivazioni hidden layer (n_hidden)\n    \"\"\"\n    n_hidden_units = w_ji.shape[1]\n    x_j = np.zeros(n_hidden_units)\n    #print(\"inside forward hidden, x_j.size: \", x_j.shape, \"w_ji.shape: \", w_ji.shape)\n    for i in range(n_hidden_units):\n        x_j[i] = sigmaf(np.dot(x_i, w_ji[:, i]))\n\n    return x_j\n\n\ndef forward_output(x_j: Array1D, w_kj: Array2D) -> Array1D:\n    \"\"\"\n    Calcola l'output layer (lineare).\n    \n    Args:\n        X1: vettore attivazioni hidden layer (n_hidden)\n        K: matrice pesi output (n_hidden, n_outputs)\n    \n    Ritorna:\n        vettore predizioni\n    \"\"\"\n    n_outputs = w_kj.shape[1]\n    x_k = np.zeros(n_outputs)\n\n    for i in range(n_outputs):\n        x_k[i] = np.dot(x_j, w_kj[:, i])\n\n    return x_k\n\n    \n    \ndef forward_all_layers(x_i: Array1D, w_ji: Array2D, w_kj: Array2D) -> tuple[Array1D, Array1D]:\n\n    x_j = forward_hidden(x_i, w_ji)\n    return forward_output(x_j, w_kj), x_j
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/staticnn/training/forward/forward_pass.py b/src/staticnn/training/forward/forward_pass.py
--- a/src/staticnn/training/forward/forward_pass.py	(revision 2525b2dda23d528628ad70037169760047ca21b8)
+++ b/src/staticnn/training/forward/forward_pass.py	(date 1764251189053)
@@ -1,6 +1,6 @@
 import numpy as np
 from src.staticnn.activationf.sigm import sigmaf
-
+from src.staticnn.activationf.relu import *
 # Tipi utili per chiarezza
 Array2D = np.ndarray
 Array1D = np.ndarray
@@ -20,8 +20,8 @@
     x_j = np.zeros(n_hidden_units)
     #print("inside forward hidden, x_j.size: ", x_j.shape, "w_ji.shape: ", w_ji.shape)
     for i in range(n_hidden_units):
-        x_j[i] = sigmaf(np.dot(x_i, w_ji[:, i]))
-
+        #x_j[i] = sigmaf(np.dot(x_i, w_ji[:, i]))
+        x_j[i] = relu(np.dot(x_i, w_ji[:, i]))
     return x_j
 
 
Index: scripts/run_training.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># MAIN PROGETTO ML\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom src.staticnn.model.fixednn import initialize_neuraln\nfrom src.staticnn.training.forward.forward_pass import *\nfrom data.utils.load_data import load_data\nfrom src.staticnn.activationf.sigm import *\nfrom src.staticnn.training.backward.backprop import *\n\n\n# Tipi utili per chiarezza\nArray2D = np.ndarray\nArray1D = np.ndarray\nArray0D = np.ndarray\n\n\ndef run_training() -> None:\n    # X: dataset input (N, 12)\n    # D: dataset target (N, 4)\n    x_i, d = load_data(\"data/training_data/ML-CUP25-TR.csv\")\n\n    print(\"x_i.shape: \", x_i.shape, \" d.shape: \", d.shape)\n    x_i = x_i.to_numpy()\n    d = d.to_numpy()\n\n    # ----------------------- INIZIALIZZA NN -------------------------\n    # x: vettore input layer\n    # w: matrice pesi input layer - hidden layer\n    # k: matrice pesi hidden layer - output layer\n    # d: vettori risultati target\n\n    x_i, w_ji, w_kj, d = initialize_neuraln(x_i, d) # <-- Inizializza NN (static)\n\n    print(\"x_i_biased.shape: \", x_i.shape, \"w_ji.shape: \", w_ji.shape, \"w_kj.shape: \", w_kj.shape)\n    # ----------------------------------------------------------------\n\n    # -------------------- FORWARD PRIMO PATTERN ---------------------    \n    #x_j = forward_hidden(x_i[0], w_ji)     # <-- Calcolo valori nodi unico hidden layer\n    #x_k = forward_output(x_j, w_kj)        # <-- Calcolo valori output\n    # ----------------------------------------------------------------\n    \n    #x_k, x_j = forward_all_layers(x_i[0], w_ji, w_kj)\n\n    eta = 0.01\n    # per un pattern\n    #w_new = w + etha * (-2 * (e[0]) * dsigmaf(x)) \n    \n    # ||||||||||||||||||||||            ONLINE          ||||||||||||||||||||||\n    #\"\"\"\n    #while True:\n    epochs = 100\n    total_error_array = np.zeros(epochs)\n\n    for epoch in range(epochs):\n\n        patterns = 500\n        total_error = 0 # total error si azzera ad ogni batch\n        for pattern in range (patterns):\n\n            x_k, x_j = forward_all_layers(x_i[pattern], w_ji, w_kj)\n            #print(\"x_k.shape: \", x_k.shape, \" x_j.shape: \", x_j.shape)\n\n            dj, dk = compute_delta_all_layers(d[pattern], x_k, w_kj, x_j, x_i[pattern], w_ji, dsigmaf)\n            print(\"delta j: \", dj, \"\\n\\ndelta k: \", dk, \"\\n\\n\\n\")\n\n            for kunit in range (w_kj.shape[1]):\n                for junit in range (w_kj.shape[0]):\n\n                    w_kj[junit][kunit] = w_kj[junit][kunit] + ( eta * dk[kunit] * x_j[junit] )\n\n                    total_error += dk[kunit]   # <-- Calcolo errore totale come sommatoria degli errori dei pattern, da poi plottare\n\n    \n            for junit in range (w_ji.shape[1]):\n                for iunit in range (w_ji.shape[0]):\n\n                    w_ji[iunit][junit] = w_ji[iunit][junit] + ( eta * dj[junit] * x_i[pattern][iunit] ) \n        \n        total_error_array[epoch] = total_error\n        print(\"!!! TOTAL ERROR: \", total_error, \" !!!\") \n\n    ep = [ x for x in range(epochs) ]\n\n    plt.plot(ep, total_error_array)\n    plt.show()\n        \n    #    if total_error <= eps:\n    #        break\n        \n        #print(\"|| Pattern number: \", pattern, \" delta_k: \", dk, \" delta_j: \", dj,\" ||\", \"\\n w_ji: \", w_ji, \"\\n w_kj: \", w_kj)\n        #print(\"|| Pattern number: \", pattern, \" delta_k: \", dk, \" delta_j: \", dj,\" ||\")\n    #\"\"\"\n\n    # ||||||||||||||||||||||            BATCH          ||||||||||||||||||||||\n    \"\"\"\n    while True: \n        \n        patterns = 500\n\n        for pattern in range(patterns):\n\n\n            x_k, x_j = forward_all_layers(x_i[pattern], w_ji, w_kj)\n            #print(\"x_k.shape: \", x_k.shape, \" x_j.shape: \", x_j.shape   )\n            dj, dk += compute_delta_all_layers(d[pattern], x_k, w_kj, x_j, x_i[pattern], w_ji, dsigmaf)\n\n            dj += \n\n\n            \n\n\n        if total_error >= e :\n\n\n            break\n\n    \"\"\"\n    print(\"x_k =\", x_k)\n\n\nif __name__ == \"__main__\":\n    run_training()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/run_training.py b/scripts/run_training.py
--- a/scripts/run_training.py	(revision 2525b2dda23d528628ad70037169760047ca21b8)
+++ b/scripts/run_training.py	(date 1764251189053)
@@ -69,7 +69,7 @@
 
                     w_kj[junit][kunit] = w_kj[junit][kunit] + ( eta * dk[kunit] * x_j[junit] )
 
-                    total_error += dk[kunit]   # <-- Calcolo errore totale come sommatoria degli errori dei pattern, da poi plottare
+                total_error += dk[kunit]   # <-- Calcolo errore totale come sommatoria degli errori dei pattern, da poi plottare
 
     
             for junit in range (w_ji.shape[1]):
Index: src/staticnn/activationf/relu.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/staticnn/activationf/relu.py b/src/staticnn/activationf/relu.py
new file mode 100644
--- /dev/null	(date 1764251189053)
+++ b/src/staticnn/activationf/relu.py	(date 1764251189053)
@@ -0,0 +1,7 @@
+import numpy as np
+
+def relu(z):
+    return np.maximum(0, z)
+
+def relu_deriv(z):
+    return (z > 0).astype(float)
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
--- /dev/null	(date 1764251189052)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1764251189052)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
--- /dev/null	(date 1764251189053)
+++ b/.idea/misc.xml	(date 1764251189053)
@@ -0,0 +1,7 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="Black">
+    <option name="sdkName" value="Python 3.14 (MLproject)" />
+  </component>
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.14 (MLproject)" project-jdk-type="Python SDK" />
+</project>
\ No newline at end of file
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100644
--- /dev/null	(date 1764251189053)
+++ b/.idea/vcs.xml	(date 1764251189053)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: .idea/MLproject.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/MLproject.iml b/.idea/MLproject.iml
new file mode 100644
--- /dev/null	(date 1764251189052)
+++ b/.idea/MLproject.iml	(date 1764251189052)
@@ -0,0 +1,7 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module version="4">
+  <component name="PyDocumentationSettings">
+    <option name="format" value="GOOGLE" />
+    <option name="myDocStringFormat" value="Google" />
+  </component>
+</module>
\ No newline at end of file
