Index: training.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nfrom models.forward_pass import forward\nfrom models.backward_pass import backward\n\n\ndef train_with_early_stopping(\n    model, optimizer,\n    X_train, Y_train, X_val, Y_val,\n    epochs=1000, batch_size=64,\n    patience=200, min_delta=1e-4,\n    verbose=True\n):\n    \"\"\"Training loop with early stopping.\"\"\"\n\n    best_val_loss = float(\"inf\")\n    best_weights = {}\n    patience_counter = 0\n    model.loss_history = []\n\n    for epoch in range(epochs):\n        perm = np.random.permutation(len(X_train))\n        X_train, Y_train = X_train[perm], Y_train[perm]\n\n        for i in range(0, len(X_train), batch_size):\n            Xb = X_train[i:i + batch_size]\n            Yb = Y_train[i:i + batch_size]\n\n            Y_pred, cache = forward(\n                Xb,\n                model.W1, model.b1,\n                model.W2, model.b2,\n                model.W3, model.b3,\n                model.W4, model.b4,\n                model.dropout_rate,\n                training=True\n            )\n\n            grads = backward(Y_pred, Yb, cache,\n                             model.W1, model.W2, model.W3, model.W4,\n                             model.lam)\n            optimizer.update(model, grads)\n\n        # Compute losses\n        Y_val_pred, _ = forward(\n            X_val,\n            model.W1, model.b1,\n            model.W2, model.b2,\n            model.W3, model.b3,\n            model.W4, model.b4,\n            model.dropout_rate,\n            training=False\n        )\n        val_loss = np.mean((Y_val_pred - Y_val) ** 2)\n        model.loss_history.append(val_loss)\n\n        if verbose and epoch % 50 == 0:\n            print(f\"Epoch {epoch}/{epochs} - Val Loss: {val_loss:.6f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss - min_delta:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_weights = {p: getattr(model, p).copy() for p in\n                            [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]}\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping triggered at epoch {epoch}.\")\n            break\n\n    for p, v in best_weights.items():\n        setattr(model, p, v)\n\n    print(f\"Training complete. Best validation loss: {best_val_loss:.6f}\")\n    return model\n
===================================================================
diff --git a/training.py b/training.py
--- a/training.py	(revision bfb5847a276b75f5ca6f477ff914c5092e6d187f)
+++ b/training.py	(date 1763223787114)
@@ -6,7 +6,7 @@
 def train_with_early_stopping(
     model, optimizer,
     X_train, Y_train, X_val, Y_val,
-    epochs=1000, batch_size=64,
+    epochs=10000, batch_size=64,
     patience=200, min_delta=1e-4,
     verbose=True
 ):
